{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo3ZOWTaDkLz"
      },
      "source": [
        "# Assignment 1: Acoustic Activity Recognition\n",
        "\n",
        "In this assignment, you will build a machine learning system for acoustic activity recognition using your smartphone microphone. You will collect a custom audio dataset, extract various features, train multiple classification models, and evaluate their performance across different conditions. Each section below corresponds to a part of the assignment, with clear instructions and placeholders where you need to implement your solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8g64rwYPfiF"
      },
      "source": [
        "## Setup and Configuration\n",
        "\n",
        "This section imports necessary libraries and mounts Google Drive for file access. We'll use libraries for audio processing (librosa), machine learning (scikit-learn), visualization (matplotlib, seaborn) and various utility functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KjwS_dXjPz1E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error mounting Google Drive. Please run this cell again.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import warnings\n",
        "import hashlib\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except:\n",
        "    print(\"Error mounting Google Drive. Please run this cell again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqOdpUqDP2-0"
      },
      "source": [
        "## Dataset Configuration\n",
        "\n",
        "Each student will have a unique dataset ID based on their name. This ID will be used to create a folder for storing your audio recordings. The configuration also defines the activities, environments, and other parameters for the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YhdjPnLAP5TT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your dataset folder ID is: 21524\n",
            "Please name your dataset folder: 21524\n"
          ]
        }
      ],
      "source": [
        "# Create a unique dataset ID based on student name\n",
        "def generate_dataset_id(name):\n",
        "    \"\"\"\n",
        "    Generate a 5-digit random number based on the hash of the name.\n",
        "    Use this as your dataset folder name.\n",
        "    \"\"\"\n",
        "    hash_object = hashlib.md5(name.encode())\n",
        "    hex_dig = hash_object.hexdigest()\n",
        "    seed = int(hex_dig, 16) % (10**8)\n",
        "    random.seed(seed)\n",
        "    return random.randint(10000, 99999)\n",
        "\n",
        "# Enter your name to generate your dataset ID\n",
        "student_name = \"Sharwin Patil\"\n",
        "dataset_id = generate_dataset_id(student_name)\n",
        "print(f\"Your dataset folder ID is: {dataset_id}\")\n",
        "print(f\"Please name your dataset folder: {dataset_id}\")\n",
        "os.makedirs(f\"{dataset_id}\", exist_ok=True)\n",
        "# Configuration\n",
        "ACTIVITIES = ['laugh', 'cough', 'clap', 'knock', 'alarm']\n",
        "ENVIRONMENTS = ['small', 'large']\n",
        "NUM_INSTANCES = 10\n",
        "SAMPLE_RATE = 16000  # Standard sample rate for audio\n",
        "WINDOW_SIZES = [1, 5, 'full']  # in seconds - 3 required window sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE92y8sysTaZ"
      },
      "source": [
        "Note: all of the input audio files should be resampled to 16kHz for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eP97UrGsTll"
      },
      "outputs": [],
      "source": [
        "# Sample function to resample the input audio to 16kHz\n",
        "def load_audio_16k(path):\n",
        "    audio, sr = librosa.load(path, SAMPLE_RATE=16000)  # Resample to 16 kHz\n",
        "    return audio, sr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQH06oryP77k"
      },
      "source": [
        "# A1.1: Data Collection (4 points)\n",
        "\n",
        "n this section, we will validate your dataset to ensure it meets the requirements. You should have recorded 5 activities (laugh, cough, clap, knock, alarm), with 10 recordings each in 2 different environments (e.g., a small room and a large room).\n",
        "\n",
        "This results in a total of 100 audio recordings (5 activities × 10 instances × 2 environments).\n",
        "\n",
        "Each recording should be about 20 seconds long and named using the format activity_environment_instanceNumber.wav.\n",
        "\n",
        "For example, cough_small_3.wav would represent the 4th coughing sample recorded in a small room.\n",
        "\n",
        "## Dataset Management Functions\n",
        "\n",
        "These functions help with listing, validating, and visualizing your audio files. The naming convention for your files should be: `activity_environment_instanceNumber.wav`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYG43wsKP7HN"
      },
      "outputs": [],
      "source": [
        "def list_audio_files(base_path):\n",
        "    \"\"\"\n",
        "    List all audio files in the dataset directory.\n",
        "\n",
        "    Args:\n",
        "        base_path: Path to the dataset directory\n",
        "\n",
        "    Returns:\n",
        "        List of audio file paths\n",
        "    \"\"\"\n",
        "    audio_files = []\n",
        "\n",
        "    # Find all WAV files in the dataset directory\n",
        "    if os.path.exists(base_path):\n",
        "        for root, dirs, files in os.walk(base_path):\n",
        "            for file in files:\n",
        "                if file.endswith('.wav'):\n",
        "                    # Check if the file follows the naming convention\n",
        "                    parts = os.path.splitext(file)[0].split('_')\n",
        "                    if len(parts) == 3:\n",
        "                        activity, environment, instance = parts\n",
        "                        # Validate activity and environment\n",
        "                        if activity in ACTIVITIES and environment in ENVIRONMENTS:\n",
        "                            # Try to convert instance to int\n",
        "                            try:\n",
        "                                instance_num = int(instance)\n",
        "                                if 0 <= instance_num < NUM_INSTANCES:\n",
        "                                    audio_files.append(os.path.join(root, file))\n",
        "                            except ValueError:\n",
        "                                pass\n",
        "\n",
        "    return audio_files\n",
        "\n",
        "def validate_dataset(audio_files):\n",
        "    \"\"\"\n",
        "    Validate that the dataset contains the required number and types of files.\n",
        "\n",
        "    Args:\n",
        "        audio_files: List of audio file paths\n",
        "\n",
        "    Returns:\n",
        "        dict: Statistics of the dataset\n",
        "        bool: True if the dataset meets requirements\n",
        "    \"\"\"\n",
        "    stats = {\n",
        "        'total_files': 0,\n",
        "        'files_by_activity': {activity: 0 for activity in ACTIVITIES},\n",
        "        'files_by_environment': {env: 0 for env in ENVIRONMENTS},\n",
        "        'activity_env_matrix': {\n",
        "            f\"{activity}_{env}\": 0\n",
        "            for activity in ACTIVITIES\n",
        "            for env in ENVIRONMENTS\n",
        "        },\n",
        "        'valid': False\n",
        "    }\n",
        "\n",
        "    # Count files by activity and environment\n",
        "    for file_path in audio_files:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        parts = os.path.splitext(file_name)[0].split('_')\n",
        "\n",
        "        if len(parts) == 3:\n",
        "            activity, environment, _ = parts\n",
        "\n",
        "            if activity in ACTIVITIES and environment in ENVIRONMENTS:\n",
        "                stats['total_files'] += 1\n",
        "                stats['files_by_activity'][activity] += 1\n",
        "                stats['files_by_environment'][environment] += 1\n",
        "                stats['activity_env_matrix'][f\"{activity}_{environment}\"] += 1\n",
        "\n",
        "    # Check if all required files are present\n",
        "    required_count = 0\n",
        "    for activity in ACTIVITIES:\n",
        "        for env in ENVIRONMENTS:\n",
        "            key = f\"{activity}_{env}\"\n",
        "            # Should have NUM_INSTANCES instances for each activity-environment pair\n",
        "            if stats['activity_env_matrix'][key] == NUM_INSTANCES:\n",
        "                required_count += 1\n",
        "\n",
        "    # Set valid flag\n",
        "    stats['valid'] = (required_count == len(ACTIVITIES) * len(ENVIRONMENTS))\n",
        "\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpOof2-CQAR7"
      },
      "outputs": [],
      "source": [
        "def display_dataset_stats(stats):\n",
        "    \"\"\"\n",
        "    Display statistics about the dataset.\n",
        "\n",
        "    Args:\n",
        "        stats: Dictionary of dataset statistics\n",
        "    \"\"\"\n",
        "    print(f\"Total files: {stats['total_files']}\")\n",
        "    print(\"\\nFiles by activity:\")\n",
        "    for activity, count in stats['files_by_activity'].items():\n",
        "        print(f\"  {activity}: {count}\")\n",
        "\n",
        "    print(\"\\nFiles by environment:\")\n",
        "    for env, count in stats['files_by_environment'].items():\n",
        "        print(f\"  {env}: {count}\")\n",
        "\n",
        "    print(\"\\nActivity-Environment Matrix:\")\n",
        "    # Create a matrix for visualization\n",
        "    matrix_data = np.zeros((len(ACTIVITIES), len(ENVIRONMENTS)))\n",
        "    for i, activity in enumerate(ACTIVITIES):\n",
        "        for j, env in enumerate(ENVIRONMENTS):\n",
        "            matrix_data[i, j] = stats['activity_env_matrix'][f\"{activity}_{env}\"]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(matrix_data, annot=True, fmt='g', cmap='viridis',\n",
        "                xticklabels=ENVIRONMENTS, yticklabels=ACTIVITIES)\n",
        "    plt.title('Number of recordings per activity and environment')\n",
        "    plt.ylabel('Activity')\n",
        "    plt.xlabel('Environment')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if stats['valid']:\n",
        "        print(\"\\n✅ Dataset meets the requirements!\")\n",
        "    else:\n",
        "        print(\"\\n❌ Dataset does not meet requirements. Please check your files.\")\n",
        "\n",
        "def plot_audio_waveform(audio_path):\n",
        "    \"\"\"\n",
        "    Plot the waveform of an audio file.\n",
        "\n",
        "    Args:\n",
        "        audio_path: Path to the audio file\n",
        "    \"\"\"\n",
        "    # Load the audio file\n",
        "    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE)\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    librosa.display.waveshow(y, sr=sr)\n",
        "    plt.title(f'Waveform: {os.path.basename(audio_path)}')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Display audio player\n",
        "    display(ipd.Audio(y, rate=sr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCrmOvmuDkL2"
      },
      "source": [
        "Now that we have the dataset management functions, let's use them to verify your dataset. The code below will list the files in your dataset folder, compute dataset statistics, and display a summary. It will also randomly select a few audio files and display their waveforms as a quick inspection of your recordings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBww8ZU7QInE"
      },
      "outputs": [],
      "source": [
        "def test_data_collection():\n",
        "    \"\"\"\n",
        "    Test function for data collection section.\n",
        "    \"\"\"\n",
        "    # TODO: Define the path to your dataset directory\n",
        "    dataset_path = f\"/content/drive/MyDrive/CSXXX/{dataset_id}\"\n",
        "\n",
        "    print(\"Checking dataset...\")\n",
        "    audio_files = list_audio_files(dataset_path)\n",
        "    stats = validate_dataset(audio_files)\n",
        "    display_dataset_stats(stats)\n",
        "\n",
        "    if stats['valid'] and len(audio_files) > 0:\n",
        "        # Display a sample of audio files\n",
        "        print(\"\\nDisplaying a sample of audio files:\")\n",
        "        sample_files = random.sample(audio_files, min(3, len(audio_files)))\n",
        "        for file in sample_files:\n",
        "            print(f\"\\nAudio file: {os.path.basename(file)}\")\n",
        "            plot_audio_waveform(file)\n",
        "\n",
        "    return stats['valid']\n",
        "\n",
        "# Run the test\n",
        "test_data_collection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLLod3n8QHbi"
      },
      "source": [
        "# A1.2: Feature Extraction (4 points)\n",
        "\n",
        "In this section, you'll implement three different audio feature extraction techniques:\n",
        "1. Fast Fourier Transform (FFT) for frequency domain analysis\n",
        "2. Mel-Frequency Cepstral Coefficients (MFCCs) for spectral features\n",
        "3. Root Mean Square (RMS) energy for amplitude/energy analysis\n",
        "\n",
        "We will implement each feature extraction in a separate function. After that, we'll integrate them into a pipeline to apply these features to the entire dataset.\n",
        "\n",
        "## Feature Extraction Functions\n",
        "\n",
        "For each feature type, complete the TODOs in the corresponding function. Each function should take an audio signal and return a feature vector (or a set of feature vectors if using windowing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdFGv6TOQMnC"
      },
      "outputs": [],
      "source": [
        " def extract_fft(audio_signal, sr=16000):\n",
        "    \"\"\"\n",
        "    Extract Fast Fourier Transform (FFT) features from an audio signal.\n",
        "\n",
        "    Args:\n",
        "        audio_signal: The audio signal\n",
        "        sr: Sample rate\n",
        "\n",
        "    Returns:\n",
        "        FFT features\n",
        "    \"\"\"\n",
        "    # TODO: Implement FFT feature extraction\n",
        "    # Your implementation should:\n",
        "    # 1. Compute the magnitude spectrum\n",
        "    # 2. Convert to power spectrum\n",
        "    # 3. Extract relevant features (e.g., mean, std, energy in frequency bands)\n",
        "    # 4. Return a one dimensional feature vector\n",
        "\n",
        "    # Placeholder return\n",
        "    return np.zeros(9)\n",
        "\n",
        "def extract_mfcc(audio_signal, sr=16000):\n",
        "    \"\"\"\n",
        "    Extract Mel-Frequency Cepstral Coefficients (MFCCs) from an audio signal.\n",
        "\n",
        "    Args:\n",
        "        audio_signal: The audio signal\n",
        "        sr: Sample rate\n",
        "\n",
        "    Returns:\n",
        "        MFCC features\n",
        "    \"\"\"\n",
        "    # TODO: Implement MFCC feature extraction\n",
        "    # Your implementation should:\n",
        "    # 1. Extract MFCCs using librosa\n",
        "    # 2. Calculate statistics for each coefficient\n",
        "    # 3. Optionally include delta and delta2 features\n",
        "    # 4. Return a one dimensional feature vector\n",
        "\n",
        "    # Placeholder return\n",
        "    return np.zeros(9)\n",
        "\n",
        "def extract_rms(audio_signal, sr=16000):\n",
        "    \"\"\"\n",
        "    Extract Root Mean Square (RMS) energy features from an audio signal.\n",
        "\n",
        "    Args:\n",
        "        audio_signal: The audio signal\n",
        "        sr: Sample rate\n",
        "\n",
        "    Returns:\n",
        "        RMS energy features\n",
        "    \"\"\"\n",
        "    # TODO: Implement RMS feature extraction\n",
        "    # Your implementation should:\n",
        "    # 1. Extract RMS energy\n",
        "    # 2. Calculate statistics (mean, std, max, min)\n",
        "    # 3. Include additional temporal features if desired\n",
        "    # 4. Return a one dimensional feature vector\n",
        "\n",
        "    # Placeholder return\n",
        "    return np.zeros(9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10ft4SV3SKAR"
      },
      "source": [
        "## Feature Processing Pipeline\n",
        "\n",
        "Now we'll integrate the above feature extraction functions into a pipeline that can process a list of audio files and output a feature matrix X, label vector y, and some metadata. This pipeline will also handle windowing: if a window size (in seconds) is provided, it will slice each audio file into windows and treat each window as a separate sample with the same label.\n",
        "\n",
        "The extract_features function (provided) calls the appropriate extract_* function based on the feature type and also performs windowing if needed. The process_dataset function uses extract_features for each file in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNJ7BEZDSKm_"
      },
      "outputs": [],
      "source": [
        "def extract_features(audio_path, feature_type='all'):\n",
        "    \"\"\"\n",
        "    Extract features from an audio file.\n",
        "\n",
        "    Args:\n",
        "        audio_path: Path to the audio file\n",
        "        feature_type: Type of features to extract ('fft', 'mfcc', 'rms', or 'all')\n",
        "\n",
        "    Returns:\n",
        "        Extracted features\n",
        "    \"\"\"\n",
        "    # Load the audio file\n",
        "    y, sr = librosa.load(audio_path, sr=SAMPLE_RATE)\n",
        "\n",
        "    # Extract features from the whole file\n",
        "    features = {}\n",
        "    if feature_type in ['fft', 'all']:\n",
        "        features['fft'] = extract_fft(y, sr)\n",
        "    if feature_type in ['mfcc', 'all']:\n",
        "        features['mfcc'] = extract_mfcc(y, sr)\n",
        "    if feature_type in ['rms', 'all']:\n",
        "        features['rms'] = extract_rms(y, sr)\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w7JUnhFSNGe"
      },
      "outputs": [],
      "source": [
        "def process_dataset(audio_files, feature_types=['fft', 'mfcc', 'rms'], window_size=None):\n",
        "    \"\"\"\n",
        "    Process the entire dataset to extract features and prepare for model training.\n",
        "\n",
        "    Args:\n",
        "        audio_files: List of audio file paths\n",
        "        feature_types: List of feature types to extract\n",
        "        window_size: Size of the window in seconds (None for whole file)\n",
        "\n",
        "    Returns:\n",
        "        X: Feature matrix\n",
        "        y: Labels\n",
        "        metadata: Additional information about the data\n",
        "    \"\"\"\n",
        "    # Initialize lists to store features and labels\n",
        "    X = []\n",
        "    y = []\n",
        "    metadata = {\n",
        "        'activity': [],\n",
        "        'environment': [],\n",
        "        'instance': [],\n",
        "        'file_path': []\n",
        "    }\n",
        "\n",
        "    # Ensure feature_types is a list\n",
        "    if not isinstance(feature_types, list):\n",
        "        feature_types = [feature_types]\n",
        "\n",
        "    # Process each audio file\n",
        "    successful_files = 0\n",
        "    for file_path in tqdm(audio_files, desc=\"Processing audio files\"):\n",
        "        # Parse filename to get activity and environment\n",
        "        file_name = os.path.basename(file_path)\n",
        "        parts = os.path.splitext(file_name)[0].split('_')\n",
        "\n",
        "        if len(parts) == 3:\n",
        "            activity, environment, instance = parts\n",
        "\n",
        "            # Extract features for each feature type separately\n",
        "            all_features = {}\n",
        "            for feat_type in feature_types:\n",
        "                # Call extract_features with a single feature type\n",
        "                result = extract_features(file_path, feature_type=feat_type, window_size=window_size)\n",
        "                if feat_type in result and result[feat_type] is not None:\n",
        "                    all_features[feat_type] = result[feat_type]\n",
        "\n",
        "            # Process based on whether we have windowed features or whole file features\n",
        "            is_windowed = False\n",
        "            for feat_type in all_features:\n",
        "                if isinstance(all_features[feat_type], list) or (isinstance(all_features[feat_type], np.ndarray) and len(all_features[feat_type].shape) > 1):\n",
        "                    is_windowed = True\n",
        "                    break\n",
        "\n",
        "            if is_windowed:\n",
        "                # Handle windowed features\n",
        "                num_windows = len(next(iter(all_features.values())))\n",
        "                for window_idx in range(num_windows):\n",
        "                    feature_vector = []\n",
        "                    for feat_type in feature_types:\n",
        "                        if feat_type in all_features:\n",
        "                            feature_vector.extend(all_features[feat_type][window_idx].flatten())\n",
        "\n",
        "                    # Only add if we have features\n",
        "                    if feature_vector:\n",
        "                        X.append(np.array(feature_vector))\n",
        "                        y.append(ACTIVITIES.index(activity))\n",
        "                        metadata['activity'].append(activity)\n",
        "                        metadata['environment'].append(environment)\n",
        "                        metadata['instance'].append(f\"{instance}_w{window_idx}\")\n",
        "                        metadata['file_path'].append(file_path)\n",
        "            else:\n",
        "                # Handle whole file features\n",
        "                feature_vector = []\n",
        "                for feat_type in feature_types:\n",
        "                    if feat_type in all_features:\n",
        "                        feature_vector.extend(all_features[feat_type].flatten())\n",
        "\n",
        "                # Only add if we have features\n",
        "                if feature_vector:\n",
        "                    X.append(np.array(feature_vector))\n",
        "                    y.append(ACTIVITIES.index(activity))\n",
        "                    metadata['activity'].append(activity)\n",
        "                    metadata['environment'].append(environment)\n",
        "                    metadata['instance'].append(instance)\n",
        "                    metadata['file_path'].append(file_path)\n",
        "\n",
        "            successful_files += 1\n",
        "\n",
        "    print(f\"Successfully processed {successful_files} out of {len(audio_files)} files\")\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    if len(X) > 0:\n",
        "        X_array = np.array(X)\n",
        "        y_array = np.array(y)\n",
        "    else:\n",
        "        print(\"WARNING: No features were extracted successfully! Creating dummy features for testing.\")\n",
        "        # Create a minimal set of dummy features to allow the rest of the pipeline to run\n",
        "        X_array = np.random.rand(len(ACTIVITIES) * len(ENVIRONMENTS), 78)  # 78 matches our feature dimension\n",
        "        y_array = np.array([i % len(ACTIVITIES) for i in range(len(ACTIVITIES) * len(ENVIRONMENTS))])\n",
        "        for i in range(len(X_array)):\n",
        "            activity = ACTIVITIES[i % len(ACTIVITIES)]\n",
        "            env = ENVIRONMENTS[i // len(ACTIVITIES) % len(ENVIRONMENTS)]\n",
        "            metadata['activity'].append(activity)\n",
        "            metadata['environment'].append(env)\n",
        "            metadata['instance'].append('0')\n",
        "            metadata['file_path'].append(f\"{activity}_{env}_0.wav\")\n",
        "\n",
        "    return X_array, y_array, metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G624CTGSWGq"
      },
      "outputs": [],
      "source": [
        "def test_feature_extraction():\n",
        "    \"\"\"\n",
        "    Test function for feature extraction section.\n",
        "    \"\"\"\n",
        "    # Define the path to your dataset directory\n",
        "    dataset_path = f\"/content/drive/MyDrive/CSXXX/{dataset_id}\"\n",
        "\n",
        "    print(\"Testing feature extraction...\")\n",
        "    audio_files = list_audio_files(dataset_path)\n",
        "\n",
        "    if len(audio_files) > 0:\n",
        "        # Test with a sample file\n",
        "        sample_file = random.choice(audio_files)\n",
        "        print(f\"Using sample file: {os.path.basename(sample_file)}\")\n",
        "\n",
        "        # Extract and visualize different feature types\n",
        "        for feature_type in ['fft', 'mfcc', 'rms']:\n",
        "            print(f\"\\nExtracting {feature_type.upper()} features...\")\n",
        "            features = extract_features(sample_file, feature_type=feature_type)\n",
        "\n",
        "        # Test dataset processing with different window sizes\n",
        "        for window_size in [None, 1]:\n",
        "            print(f\"\\nProcessing dataset with window_size={window_size}...\")\n",
        "            X, y, metadata = process_dataset(\n",
        "                audio_files[:10],  # Use a subset for testing\n",
        "                feature_types=['mfcc'],  # Use only one feature type for speed\n",
        "                window_size=window_size\n",
        "            )\n",
        "            print(f\"Feature matrix shape: {X.shape}\")\n",
        "            print(f\"Labels shape: {y.shape}\")\n",
        "            print(f\"Unique activities: {np.unique(metadata['activity'])}\")\n",
        "            print(f\"Unique environments: {np.unique(metadata['environment'])}\")\n",
        "    else:\n",
        "        print(\"No audio files found. Please check your dataset.\")\n",
        "\n",
        "    return len(audio_files) > 0\n",
        "\n",
        "# Run the test\n",
        "test_feature_extraction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be4vg6WcSr2o"
      },
      "source": [
        "# A1.3: Model Training and Comparison (4 points)\n",
        "\n",
        "In this section, you'll implement and compare different classification algorithms for activity recognition. We will perform cross-environment evaluation: train models on data from one environment and test on the other to see how well they generalize.\n",
        "\n",
        "## Model Training and Evaluation Functions\n",
        "\n",
        "These functions implement environment-based data splitting, model training, and comparison across different algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S23-nTzJSshC"
      },
      "outputs": [],
      "source": [
        "def train_test_environment_split(X, y, metadata, train_env, test_env):\n",
        "    \"\"\"\n",
        "    Split the data based on environment for cross-environment evaluation.\n",
        "\n",
        "    Args:\n",
        "        X: Feature matrix\n",
        "        y: Labels\n",
        "        metadata: Additional information about the data\n",
        "        train_env: Environment to use for training\n",
        "        test_env: Environment to use for testing\n",
        "\n",
        "    Returns:\n",
        "        X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    # TODO: Implement environment-based train/test split\n",
        "    # Your implementation should:\n",
        "    # 1. Split the data for training and testing\n",
        "    # 2. Handle cases where not all classes are represented\n",
        "    # 3. Return X_train, X_test, y_train, y_test\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vFllVpxSvp_"
      },
      "outputs": [],
      "source": [
        "def test_model_training():\n",
        "    \"\"\"\n",
        "    Test function for model training and comparison section.\n",
        "    \"\"\"\n",
        "    # Define the path to your dataset directory\n",
        "    dataset_path = f\"/content/drive/MyDrive/CSXXX/{dataset_id}\"\n",
        "\n",
        "    print(\"Testing model training and comparison...\")\n",
        "    audio_files = list_audio_files(dataset_path)\n",
        "\n",
        "    if len(audio_files) > 0:\n",
        "        # Select multiple files per activity and environment\n",
        "        selected_files = []\n",
        "        for activity in ACTIVITIES:\n",
        "            for env in ENVIRONMENTS:\n",
        "                matching_files = [f for f in audio_files\n",
        "                                 if os.path.basename(f).startswith(f\"{activity}_{env}\")]\n",
        "                if matching_files:\n",
        "                    selected_files.extend(matching_files[:3])  # Take up to 3 files per category\n",
        "\n",
        "        # TODO: extract features\n",
        "        print(\"Extracting features\")\n",
        "\n",
        "        # Check class distribution\n",
        "        print(f\"Feature matrix shape: {X.shape}\")\n",
        "        print(f\"Class distribution: {np.bincount(y)}\")\n",
        "\n",
        "        # TODO: train/test split\n",
        "\n",
        "        # Test model training and evaluation with a simple model\n",
        "        print(\"\\nTesting model training and evaluation...\")\n",
        "        model = RandomForestClassifier(n_estimators=10, random_state=42)  # Use a small model for speed\n",
        "        trained_model, accuracy = train_and_evaluate_model(\n",
        "            model, X_train, X_test, y_train, y_test\n",
        "        )\n",
        "        print(f\"Test accuracy: {accuracy:.2f}\")\n",
        "\n",
        "        return True\n",
        "    else:\n",
        "        print(\"No audio files found. Please check your dataset.\")\n",
        "        return False\n",
        "\n",
        "# Run the test\n",
        "test_model_training()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S4gcCP1JT9I"
      },
      "source": [
        "TODO:\n",
        "\n",
        "In your report, discuss (1-2 paragraphs):\n",
        "The signal processing parameters you used (e.g., sampling rate, frame size, hop length).\n",
        "\n",
        "How these different features capture various aspects of the audio signal\n",
        "Any preprocessing steps applied (e.g., noise reduction, normalization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ7CLcyuS7Y2"
      },
      "source": [
        "# A1.4: Time Window Analysis (3 points)\n",
        "\n",
        "This section analyzes how different time window sizes affect classification performance. We will extract features using different window lengths and see how the model accuracies change. Specifically, we'll consider window sizes of 1 second, 2 seconds, 5 seconds, 10 seconds, and the entire 20-second file (treated as one window).\n",
        "\n",
        "For each window size, we'll perform the same model training and cross-environment evaluation as in A1.3 for each classifier, then visualize the results in a single plot.\n",
        "\n",
        "TODO:\n",
        " - Update the functions above to process the dataset with different window sizes, compare model performances, and visualize the results.\n",
        " - Create a line plot showing:\n",
        "  - x-axis: Window size (1s, 5s, whole file)\n",
        "  - y-axis: Average accuracy across both environments\n",
        "  - Separate lines for each classifier (color-coded: Random Forest in green, - Logistic Regression in yellow, SVM in blue, MLP in red)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqr80lgDKmpA"
      },
      "outputs": [],
      "source": [
        "# TODO: Time Window Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkXYHI-XTC2W"
      },
      "source": [
        "# A1.5: Hyperparameter Optimization (2 points)\n",
        "\n",
        "After identifying the best-performing model and feature/window configuration, we will tune that model’s hyperparameters to improve performance. In this section, you'll perform a systematic search (e.g., grid search) over at least two hyperparameters of your chosen model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OSJhlwhJ2We"
      },
      "outputs": [],
      "source": [
        "# TODO: Hyperparameter Optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAeZ1W6ZJ3DR"
      },
      "source": [
        "TODO:\n",
        "\n",
        "In your writeup, report the optimal hyperparameter values, and discuss how different hyperparameter settings affected the model performance (1 paragraph)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czFJFoEHTIQe"
      },
      "source": [
        "# A1.6: Model Evaluation on Holdout Data (3 points)\n",
        "\n",
        "In the final section, you'll evaluate your optimized model on a holdout dataset. The goal is to see how well your model generalizes to entirely new data. You will load the selected datasets and labels, apply feature extraction methods using a proper window size, and generate a confusion matrix for the holdout results by analyzing which activities were most often misclassified.\n",
        "\n",
        "Your dataset contains 5 audio files per activity class, resulting in total of 25 activities. Each audio file results in one prediction. The dataset can be found in link below:\n",
        "\n",
        "https://www.dropbox.com/scl/fo/y31s1uf5ili214c7bdmkf/ACydLoc4VhwqGZ5esrlGihA?rlkey=i62hjm4bw5beyd31wfimd4mau&dl=0\n",
        "\n",
        "Output: 25 total prediction across 5 classes for the confusion matrix.\n",
        "\n",
        "Download and listen to the audio files. Can you improve the accuracy?\n",
        "\n",
        "Try intelligent filtering to remove background noise. Think about automatically taking only take frame segments that have audio above a certain noise threshold. Is there any better way to do windowing?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TCbjFdmDkL4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your dataset\n",
        "dataset_path = '/content/drive/MyDrive/YOUR_FOLDER_NAME'  # TODO: Replace with your folder path\n",
        "\n",
        "# Example for organizing the download files:\n",
        "# dataset_path/\n",
        "# ├── class1(laugh)/\n",
        "# │   ├── file1.wav\n",
        "# │   ├── file2.wav\n",
        "# │   └── ...\n",
        "# ├── class2()/\n",
        "# └── ...\n",
        "# Each class folder should contain 5 audio files\n",
        "\n",
        "# TODO: load dataset, perform feature extraction\n",
        "# Steps:\n",
        "#   1. For each class folder, load the datasets and labels.\n",
        "#   2. Extract features from the 5 audio files.\n",
        "#   3. Run predictions using your trained model. Each file should produce only 1 predication.\n",
        "#   4. Store true labels and predicted labels, resulting in 25 total predictions\n",
        "#   5. Plot a confusion matrix using these labels.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2eDgRMyIv8L"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
